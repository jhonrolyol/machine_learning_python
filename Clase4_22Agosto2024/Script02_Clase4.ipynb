{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvdo9KcXGNcOjr1pT8+YpM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7HLYh9A-dAS","executionInfo":{"status":"ok","timestamp":1724378686430,"user_tz":300,"elapsed":10264,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"65fd373a-a983-4fe4-f3da-0003f1caa63e"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 119390 entries, 0 to 119389\n","Data columns (total 32 columns):\n"," #   Column                          Non-Null Count   Dtype  \n","---  ------                          --------------   -----  \n"," 0   hotel                           119390 non-null  object \n"," 1   is_canceled                     119390 non-null  int64  \n"," 2   lead_time                       119390 non-null  int64  \n"," 3   arrival_date_year               119390 non-null  int64  \n"," 4   arrival_date_month              119390 non-null  object \n"," 5   arrival_date_week_number        119390 non-null  int64  \n"," 6   arrival_date_day_of_month       119390 non-null  int64  \n"," 7   stays_in_weekend_nights         119390 non-null  int64  \n"," 8   stays_in_week_nights            119390 non-null  int64  \n"," 9   adults                          119390 non-null  int64  \n"," 10  children                        119386 non-null  float64\n"," 11  babies                          119390 non-null  int64  \n"," 12  meal                            119390 non-null  object \n"," 13  country                         118902 non-null  object \n"," 14  market_segment                  119390 non-null  object \n"," 15  distribution_channel            119390 non-null  object \n"," 16  is_repeated_guest               119390 non-null  int64  \n"," 17  previous_cancellations          119390 non-null  int64  \n"," 18  previous_bookings_not_canceled  119390 non-null  int64  \n"," 19  reserved_room_type              119390 non-null  object \n"," 20  assigned_room_type              119390 non-null  object \n"," 21  booking_changes                 119390 non-null  int64  \n"," 22  deposit_type                    119390 non-null  object \n"," 23  agent                           103050 non-null  float64\n"," 24  company                         6797 non-null    float64\n"," 25  days_in_waiting_list            119390 non-null  int64  \n"," 26  customer_type                   119390 non-null  object \n"," 27  adr                             119390 non-null  float64\n"," 28  required_car_parking_spaces     119390 non-null  int64  \n"," 29  total_of_special_requests       119390 non-null  int64  \n"," 30  reservation_status              119390 non-null  object \n"," 31  reservation_status_date         119390 non-null  object \n","dtypes: float64(4), int64(16), object(12)\n","memory usage: 29.1+ MB\n","None\n"]}],"source":["# Modulos a cargar\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Machine Learning\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import metrics\n","\n","# Datos\n","df = pd.read_csv(\"https://raw.githubusercontent.com/robintux/Datasets4StackOverFlowQuestions/master/hotel.csv\")\n","print(df.info())"]},{"cell_type":"code","source":["# Documentacion de la clase DecisionTreeClassifier\n","help(DecisionTreeClassifier)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Iwpis5gS-uYM","executionInfo":{"status":"ok","timestamp":1724378723043,"user_tz":300,"elapsed":329,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"c963c20e-e3f5-409a-bc8c-588cb9e31f6f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n","\n","class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n"," |  DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n"," |  \n"," |  A decision tree classifier.\n"," |  \n"," |  Read more in the :ref:`User Guide <tree>`.\n"," |  \n"," |  Parameters\n"," |  ----------\n"," |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n"," |      The function to measure the quality of a split. Supported criteria are\n"," |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n"," |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n"," |  \n"," |  splitter : {\"best\", \"random\"}, default=\"best\"\n"," |      The strategy used to choose the split at each node. Supported\n"," |      strategies are \"best\" to choose the best split and \"random\" to choose\n"," |      the best random split.\n"," |  \n"," |  max_depth : int, default=None\n"," |      The maximum depth of the tree. If None, then nodes are expanded until\n"," |      all leaves are pure or until all leaves contain less than\n"," |      min_samples_split samples.\n"," |  \n"," |  min_samples_split : int or float, default=2\n"," |      The minimum number of samples required to split an internal node:\n"," |  \n"," |      - If int, then consider `min_samples_split` as the minimum number.\n"," |      - If float, then `min_samples_split` is a fraction and\n"," |        `ceil(min_samples_split * n_samples)` are the minimum\n"," |        number of samples for each split.\n"," |  \n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |  \n"," |  min_samples_leaf : int or float, default=1\n"," |      The minimum number of samples required to be at a leaf node.\n"," |      A split point at any depth will only be considered if it leaves at\n"," |      least ``min_samples_leaf`` training samples in each of the left and\n"," |      right branches.  This may have the effect of smoothing the model,\n"," |      especially in regression.\n"," |  \n"," |      - If int, then consider `min_samples_leaf` as the minimum number.\n"," |      - If float, then `min_samples_leaf` is a fraction and\n"," |        `ceil(min_samples_leaf * n_samples)` are the minimum\n"," |        number of samples for each node.\n"," |  \n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |  \n"," |  min_weight_fraction_leaf : float, default=0.0\n"," |      The minimum weighted fraction of the sum total of weights (of all\n"," |      the input samples) required to be at a leaf node. Samples have\n"," |      equal weight when sample_weight is not provided.\n"," |  \n"," |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n"," |      The number of features to consider when looking for the best split:\n"," |  \n"," |          - If int, then consider `max_features` features at each split.\n"," |          - If float, then `max_features` is a fraction and\n"," |            `max(1, int(max_features * n_features_in_))` features are considered at\n"," |            each split.\n"," |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n"," |          - If \"log2\", then `max_features=log2(n_features)`.\n"," |          - If None, then `max_features=n_features`.\n"," |  \n"," |      Note: the search for a split does not stop until at least one\n"," |      valid partition of the node samples is found, even if it requires to\n"," |      effectively inspect more than ``max_features`` features.\n"," |  \n"," |  random_state : int, RandomState instance or None, default=None\n"," |      Controls the randomness of the estimator. The features are always\n"," |      randomly permuted at each split, even if ``splitter`` is set to\n"," |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n"," |      select ``max_features`` at random at each split before finding the best\n"," |      split among them. But the best found split may vary across different\n"," |      runs, even if ``max_features=n_features``. That is the case, if the\n"," |      improvement of the criterion is identical for several splits and one\n"," |      split has to be selected at random. To obtain a deterministic behaviour\n"," |      during fitting, ``random_state`` has to be fixed to an integer.\n"," |      See :term:`Glossary <random_state>` for details.\n"," |  \n"," |  max_leaf_nodes : int, default=None\n"," |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n"," |      Best nodes are defined as relative reduction in impurity.\n"," |      If None then unlimited number of leaf nodes.\n"," |  \n"," |  min_impurity_decrease : float, default=0.0\n"," |      A node will be split if this split induces a decrease of the impurity\n"," |      greater than or equal to this value.\n"," |  \n"," |      The weighted impurity decrease equation is the following::\n"," |  \n"," |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n"," |                              - N_t_L / N_t * left_impurity)\n"," |  \n"," |      where ``N`` is the total number of samples, ``N_t`` is the number of\n"," |      samples at the current node, ``N_t_L`` is the number of samples in the\n"," |      left child, and ``N_t_R`` is the number of samples in the right child.\n"," |  \n"," |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n"," |      if ``sample_weight`` is passed.\n"," |  \n"," |      .. versionadded:: 0.19\n"," |  \n"," |  class_weight : dict, list of dict or \"balanced\", default=None\n"," |      Weights associated with classes in the form ``{class_label: weight}``.\n"," |      If None, all classes are supposed to have weight one. For\n"," |      multi-output problems, a list of dicts can be provided in the same\n"," |      order as the columns of y.\n"," |  \n"," |      Note that for multioutput (including multilabel) weights should be\n"," |      defined for each class of every column in its own dict. For example,\n"," |      for four-class multilabel classification weights should be\n"," |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n"," |      [{1:1}, {2:5}, {3:1}, {4:1}].\n"," |  \n"," |      The \"balanced\" mode uses the values of y to automatically adjust\n"," |      weights inversely proportional to class frequencies in the input data\n"," |      as ``n_samples / (n_classes * np.bincount(y))``\n"," |  \n"," |      For multi-output, the weights of each column of y will be multiplied.\n"," |  \n"," |      Note that these weights will be multiplied with sample_weight (passed\n"," |      through the fit method) if sample_weight is specified.\n"," |  \n"," |  ccp_alpha : non-negative float, default=0.0\n"," |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n"," |      subtree with the largest cost complexity that is smaller than\n"," |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n"," |      :ref:`minimal_cost_complexity_pruning` for details.\n"," |  \n"," |      .. versionadded:: 0.22\n"," |  \n"," |  Attributes\n"," |  ----------\n"," |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n"," |      The classes labels (single output problem),\n"," |      or a list of arrays of class labels (multi-output problem).\n"," |  \n"," |  feature_importances_ : ndarray of shape (n_features,)\n"," |      The impurity-based feature importances.\n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance [4]_.\n"," |  \n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |  \n"," |  max_features_ : int\n"," |      The inferred value of max_features.\n"," |  \n"," |  n_classes_ : int or list of int\n"," |      The number of classes (for single output problems),\n"," |      or a list containing the number of classes for each\n"," |      output (for multi-output problems).\n"," |  \n"," |  n_features_in_ : int\n"," |      Number of features seen during :term:`fit`.\n"," |  \n"," |      .. versionadded:: 0.24\n"," |  \n"," |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n"," |      Names of features seen during :term:`fit`. Defined only when `X`\n"," |      has feature names that are all strings.\n"," |  \n"," |      .. versionadded:: 1.0\n"," |  \n"," |  n_outputs_ : int\n"," |      The number of outputs when ``fit`` is performed.\n"," |  \n"," |  tree_ : Tree instance\n"," |      The underlying Tree object. Please refer to\n"," |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n"," |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n"," |      for basic usage of these attributes.\n"," |  \n"," |  See Also\n"," |  --------\n"," |  DecisionTreeRegressor : A decision tree regressor.\n"," |  \n"," |  Notes\n"," |  -----\n"," |  The default values for the parameters controlling the size of the trees\n"," |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n"," |  unpruned trees which can potentially be very large on some data sets. To\n"," |  reduce memory consumption, the complexity and size of the trees should be\n"," |  controlled by setting those parameter values.\n"," |  \n"," |  The :meth:`predict` method operates using the :func:`numpy.argmax`\n"," |  function on the outputs of :meth:`predict_proba`. This means that in\n"," |  case the highest predicted probabilities are tied, the classifier will\n"," |  predict the tied class with the lowest index in :term:`classes_`.\n"," |  \n"," |  References\n"," |  ----------\n"," |  \n"," |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n"," |  \n"," |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n"," |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n"," |  \n"," |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n"," |         Learning\", Springer, 2009.\n"," |  \n"," |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n"," |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n"," |  \n"," |  Examples\n"," |  --------\n"," |  >>> from sklearn.datasets import load_iris\n"," |  >>> from sklearn.model_selection import cross_val_score\n"," |  >>> from sklearn.tree import DecisionTreeClassifier\n"," |  >>> clf = DecisionTreeClassifier(random_state=0)\n"," |  >>> iris = load_iris()\n"," |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n"," |  ...                             # doctest: +SKIP\n"," |  ...\n"," |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n"," |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n"," |  \n"," |  Method resolution order:\n"," |      DecisionTreeClassifier\n"," |      sklearn.base.ClassifierMixin\n"," |      BaseDecisionTree\n"," |      sklearn.base.MultiOutputMixin\n"," |      sklearn.base.BaseEstimator\n"," |      sklearn.utils._metadata_requests._MetadataRequester\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  fit(self, X, y, sample_weight=None, check_input=True)\n"," |      Build a decision tree classifier from the training set (X, y).\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The training input samples. Internally, it will be converted to\n"," |          ``dtype=np.float32`` and if a sparse matrix is provided\n"," |          to a sparse ``csc_matrix``.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The target values (class labels) as integers or strings.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights. If None, then samples are equally weighted. Splits\n"," |          that would create child nodes with net zero or negative weight are\n"," |          ignored while searching for a split in each node. Splits are also\n"," |          ignored if they would result in any single class carrying a\n"," |          negative weight in either child node.\n"," |      \n"," |      check_input : bool, default=True\n"," |          Allow to bypass several input checking.\n"," |          Don't use this parameter unless you know what you're doing.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : DecisionTreeClassifier\n"," |          Fitted estimator.\n"," |  \n"," |  predict_log_proba(self, X)\n"," |      Predict class log-probabilities of the input samples X.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, it will be converted to\n"," |          ``dtype=np.float32`` and if a sparse matrix is provided\n"," |          to a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n"," |          The class log-probabilities of the input samples. The order of the\n"," |          classes corresponds to that in the attribute :term:`classes_`.\n"," |  \n"," |  predict_proba(self, X, check_input=True)\n"," |      Predict class probabilities of the input samples X.\n"," |      \n"," |      The predicted class probability is the fraction of samples of the same\n"," |      class in a leaf.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, it will be converted to\n"," |          ``dtype=np.float32`` and if a sparse matrix is provided\n"," |          to a sparse ``csr_matrix``.\n"," |      \n"," |      check_input : bool, default=True\n"," |          Allow to bypass several input checking.\n"," |          Don't use this parameter unless you know what you're doing.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n"," |          The class probabilities of the input samples. The order of the\n"," |          classes corresponds to that in the attribute :term:`classes_`.\n"," |  \n"," |  set_fit_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, check_input: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.tree._classes.DecisionTreeClassifier\n"," |      Request metadata passed to the ``fit`` method.\n"," |      \n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      The options for each parameter are:\n"," |      \n"," |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n"," |      \n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n"," |      \n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |      \n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |      \n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |      \n"," |      .. versionadded:: 1.3\n"," |      \n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      check_input : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``check_input`` parameter in ``fit``.\n"," |      \n"," |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |  \n"," |  set_predict_proba_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, check_input: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.tree._classes.DecisionTreeClassifier\n"," |      Request metadata passed to the ``predict_proba`` method.\n"," |      \n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      The options for each parameter are:\n"," |      \n"," |      - ``True``: metadata is requested, and passed to ``predict_proba`` if provided. The request is ignored if metadata is not provided.\n"," |      \n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict_proba``.\n"," |      \n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |      \n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |      \n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |      \n"," |      .. versionadded:: 1.3\n"," |      \n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      check_input : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``check_input`` parameter in ``predict_proba``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |  \n"," |  set_predict_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, check_input: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.tree._classes.DecisionTreeClassifier\n"," |      Request metadata passed to the ``predict`` method.\n"," |      \n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      The options for each parameter are:\n"," |      \n"," |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n"," |      \n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n"," |      \n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |      \n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |      \n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |      \n"," |      .. versionadded:: 1.3\n"," |      \n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      check_input : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``check_input`` parameter in ``predict``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |  \n"," |  set_score_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.tree._classes.DecisionTreeClassifier\n"," |      Request metadata passed to the ``score`` method.\n"," |      \n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      The options for each parameter are:\n"," |      \n"," |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n"," |      \n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n"," |      \n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |      \n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |      \n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |      \n"," |      .. versionadded:: 1.3\n"," |      \n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight`` parameter in ``score``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __abstractmethods__ = frozenset()\n"," |  \n"," |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the mean accuracy on the given test data and labels.\n"," |      \n"," |      In multi-label classification, this is the subset accuracy\n"," |      which is a harsh metric since you require for each sample that\n"," |      each label set be correctly predicted.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True labels for `X`.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from BaseDecisionTree:\n"," |  \n"," |  apply(self, X, check_input=True)\n"," |      Return the index of the leaf that each sample is predicted as.\n"," |      \n"," |      .. versionadded:: 0.17\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, it will be converted to\n"," |          ``dtype=np.float32`` and if a sparse matrix is provided\n"," |          to a sparse ``csr_matrix``.\n"," |      \n"," |      check_input : bool, default=True\n"," |          Allow to bypass several input checking.\n"," |          Don't use this parameter unless you know what you're doing.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      X_leaves : array-like of shape (n_samples,)\n"," |          For each datapoint x in X, return the index of the leaf x\n"," |          ends up in. Leaves are numbered within\n"," |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n"," |          numbering.\n"," |  \n"," |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n"," |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n"," |      \n"," |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n"," |      process.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The training input samples. Internally, it will be converted to\n"," |          ``dtype=np.float32`` and if a sparse matrix is provided\n"," |          to a sparse ``csc_matrix``.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The target values (class labels) as integers or strings.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights. If None, then samples are equally weighted. Splits\n"," |          that would create child nodes with net zero or negative weight are\n"," |          ignored while searching for a split in each node. Splits are also\n"," |          ignored if they would result in any single class carrying a\n"," |          negative weight in either child node.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      ccp_path : :class:`~sklearn.utils.Bunch`\n"," |          Dictionary-like object, with the following attributes.\n"," |      \n"," |          ccp_alphas : ndarray\n"," |              Effective alphas of subtree during pruning.\n"," |      \n"," |          impurities : ndarray\n"," |              Sum of the impurities of the subtree leaves for the\n"," |              corresponding alpha value in ``ccp_alphas``.\n"," |  \n"," |  decision_path(self, X, check_input=True)\n"," |      Return the decision path in the tree.\n"," |      \n"," |      .. versionadded:: 0.18\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, it will be converted to\n"," |          ``dtype=np.float32`` and if a sparse matrix is provided\n"," |          to a sparse ``csr_matrix``.\n"," |      \n"," |      check_input : bool, default=True\n"," |          Allow to bypass several input checking.\n"," |          Don't use this parameter unless you know what you're doing.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      indicator : sparse matrix of shape (n_samples, n_nodes)\n"," |          Return a node indicator CSR matrix where non zero elements\n"," |          indicates that the samples goes through the nodes.\n"," |  \n"," |  get_depth(self)\n"," |      Return the depth of the decision tree.\n"," |      \n"," |      The depth of a tree is the maximum distance between the root\n"," |      and any leaf.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self.tree_.max_depth : int\n"," |          The maximum depth of the tree.\n"," |  \n"," |  get_n_leaves(self)\n"," |      Return the number of leaves of the decision tree.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self.tree_.n_leaves : int\n"," |          Number of leaves.\n"," |  \n"," |  predict(self, X, check_input=True)\n"," |      Predict class or regression value for X.\n"," |      \n"," |      For a classification model, the predicted class for each sample in X is\n"," |      returned. For a regression model, the predicted value based on X is\n"," |      returned.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, it will be converted to\n"," |          ``dtype=np.float32`` and if a sparse matrix is provided\n"," |          to a sparse ``csr_matrix``.\n"," |      \n"," |      check_input : bool, default=True\n"," |          Allow to bypass several input checking.\n"," |          Don't use this parameter unless you know what you're doing.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The predicted classes, or the predict values.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from BaseDecisionTree:\n"," |  \n"," |  feature_importances_\n"," |      Return the feature importances.\n"," |      \n"," |      The importance of a feature is computed as the (normalized) total\n"," |      reduction of the criterion brought by that feature.\n"," |      It is also known as the Gini importance.\n"," |      \n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      feature_importances_ : ndarray of shape (n_features,)\n"," |          Normalized total reduction of criteria by feature\n"," |          (Gini importance).\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |  \n"," |  __getstate__(self)\n"," |  \n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |  \n"," |  __setstate__(self, state)\n"," |  \n"," |  __sklearn_clone__(self)\n"," |  \n"," |  get_params(self, deep=True)\n"," |      Get parameters for this estimator.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      deep : bool, default=True\n"," |          If True, will return the parameters for this estimator and\n"," |          contained subobjects that are estimators.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      params : dict\n"," |          Parameter names mapped to their values.\n"," |  \n"," |  set_params(self, **params)\n"," |      Set the parameters of this estimator.\n"," |      \n"," |      The method works on simple estimators as well as on nested objects\n"," |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n"," |      parameters of the form ``<component>__<parameter>`` so that it's\n"," |      possible to update each component of a nested object.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      **params : dict\n"," |          Estimator parameters.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : estimator instance\n"," |          Estimator instance.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n"," |  \n"," |  get_metadata_routing(self)\n"," |      Get metadata routing of this object.\n"," |      \n"," |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      routing : MetadataRequest\n"," |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n"," |          routing information.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n"," |  \n"," |  __init_subclass__(**kwargs) from abc.ABCMeta\n"," |      Set the ``set_{method}_request`` methods.\n"," |      \n"," |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n"," |      looks for the information available in the set default values which are\n"," |      set using ``__metadata_request__*`` class attributes, or inferred\n"," |      from method signatures.\n"," |      \n"," |      The ``__metadata_request__*`` class attributes are used when a method\n"," |      does not explicitly accept a metadata through its arguments or if the\n"," |      developer would like to specify a request value for those metadata\n"," |      which are different from the default ``None``.\n"," |      \n"," |      References\n"," |      ----------\n"," |      .. [1] https://www.python.org/dev/peps/pep-0487\n","\n"]}]},{"cell_type":"code","source":["# Tomemos la decision gerencial de eliminar todas las columnas con valores faltantes\n","ColumnasEliminar = (df.isnull().sum().sort_values(ascending=False)*100/df.shape[0]).index[:4]\n","data = df.drop(ColumnasEliminar, axis = 1)\n","# data : todas las variables sin valores faltantes\n","\n","# Consideremos a las variables de naturaleza cuantitativa\n","data1 = data.select_dtypes(include = [\"float64\", \"int64\"])\n","# data1 : Todas las variables de naturaleza cuantitativa\n","\n","# Definicion de las variables independientes (X) y la variable dependiente\n","y = data1.is_canceled\n","X = data1.drop(\"is_canceled\", axis = 1)"],"metadata":{"id":"D02x5sDuBW7G","executionInfo":{"status":"ok","timestamp":1724379484339,"user_tz":300,"elapsed":868,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Implementar una funcion que ajuste un modelo de tipo arbol de decision cuya entrada\n","# sea el valor asignado al argumento criterion de la clase DecisionTreeClassifier\n","def Estabilidad_DT(criterio):\n","  # particionado\n","  Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, train_size = 0.90,\n","                                                  stratify = y)\n","\n","  # Instanciamos la clase a modelar\n","  ModelDT_Base = DecisionTreeClassifier(criterion = criterio)\n","\n","  # Ajustamos la instancia a modelar usando el subconjunto de entrenamiento\n","  ModelDT_Base.fit(Xtrain,ytrain)\n","\n","  # Score\n","  R2_DT_Base = ModelDT_Base.score(Xtrain, ytrain)\n","\n","  # KPI\n","  y_pronostico_dt_base = ModelDT_Base.predict(Xtest)\n","  Acc_DT_Base = metrics.accuracy_score(ytest, y_pronostico_dt_base)*100\n","\n","  return (R2_DT_Base, Acc_DT_Base)"],"metadata":{"id":"xL2PqewuAywZ","executionInfo":{"status":"ok","timestamp":1724379753446,"user_tz":300,"elapsed":408,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Prueba\n","Estabilidad_DT(\"gini\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vj90z-XcChoW","executionInfo":{"status":"ok","timestamp":1724379758048,"user_tz":300,"elapsed":1165,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"4d1edc74-766e-463b-dfef-a2a2684398b0"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.9896604033466417, 80.69352542088953)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["arg_criterion = [\"gini\", \"entropy\", \"log_loss\"]\n","\n","ListaR2_c = []\n","ListaAcc_c = []\n","\n","for c in arg_criterion:\n","  ListaR2_Crit = []\n","  ListaAcc_crit = []\n","  for exp in range(1500):\n","    r2, acc = Estabilidad_DT(c)\n","    ListaR2_Crit.append(r2)\n","    ListaAcc_crit.append(acc)\n","  ListaR2_c.append(ListaR2_Crit)\n","  ListaAcc_c.append(ListaAcc_crit)\n","\n","# ListaR2_c, ListaAcc_c : Listas compuesta por 3 listas. Cada una de las cuales posee 1500 elemento.\n","# Calcular la varianza de cada una de las listas que componen a  ListaR2_c y ListaAcc_c"],"metadata":{"id":"IEe8fUH5DXZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Una vez terminado el analisis de la celda anterior.\n","# Y siendo conciente de los valores que pueden tomar estos indicadores de calidad.\n","# Concluir : De que los resultado obtenido no son suficientemente satisfactorios\n","\n","# Luego : Realizar un barrido de hiperparametros\n","\n","# Carguemos la clase que se va encargar de realizar el barrido de hiperparametros\n","from sklearn.model_selection import GridSearchCV\n"],"metadata":{"id":"WiWvMOnRFcvB","executionInfo":{"status":"ok","timestamp":1724380612845,"user_tz":300,"elapsed":327,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Analicemos que hiperparametros vamor a tunear de la clase DecisionTreeClassifier\n","# help(DecisionTreeClassifier)\n","\n","# Voy a considerar a los siguientes parametros :\n","  # criterion : [\"gini\", \"entropy\", \"log_loss\"],\n","  # splitter : [\"best\", \"random\"]\n","  # max_depth : int\n","  # ccp_alpha : non-negative float, default=0.0\n","\n","\n","# Definamos el diccionario de hiperparametros\n","DictHP_DT = {\n","    \"criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n","    \"splitter\" : [\"best\", \"random\"],\n","    \"max_depth\" : [5,10,15,25,30,45,55,75,99, 120],\n","    \"ccp_alpha\" : np.linspace(0.0001, 20, 200)\n","}\n","\n","# Instanciamos el modelo base\n","Mod_DT = DecisionTreeClassifier()\n","\n","# Configuramos el barrido de HP\n","Mod_DT_GS = GridSearchCV(estimator = Mod_DT,\n","                         param_grid = DictHP_DT,\n","                         cv = 4,\n","                         scoring = \"accuracy\",\n","                         verbose = 3,\n","                         n_jobs=-1)\n","\n","# Procedimiento pesado : Ajuste del GS\n","Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, train_size = 0.90,stratify = y)\n","Historia_GS = Mod_DT_GS.fit(Xtrain,ytrain)"],"metadata":{"collapsed":true,"id":"z44Rc8B7GBc9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AZQe4v1AHK2z","executionInfo":{"status":"ok","timestamp":1724381211108,"user_tz":300,"elapsed":308,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":14,"outputs":[]}]}